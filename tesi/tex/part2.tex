\part{Performance Analysis}

\chapter{Performance Analisys Introduction}
\emph{"Every database vendor cites benchmark data that proves its database is the fastest, and each vendor chooses the benchmark test that presents its product in the most favorable light"}\cite{burleson}.

In this chapter we will introduce database performance analysis' problem, talking about how to measure the performance, how to understand who is the fastest and how to choose the best database. This preface is not specific to in-memory databases, but can be generalized for every DBMS.
	
	\section{Impartiality Problem}
The most important problem related to performance analysis, and benchmarking, is the validity, understood as impartiality, of the results. 
		
		\subsection{Benchmark History}
The first benchmarks were developed internally by each vendor to compare their database's performance against the competitors. But when these results were published, they weren't considered reliable, because there was an evident conflict of interest between the vendor and its database \cite{gray}.
	
		\subsubsection{Benchmark Wars}
Also when benchmarks were published by third parties, or even by other competitors, the results were always discredited by loser vendors, who complained for the numbers, starting often a benchmark war. 

A benchmark war started when a loser of an important and visible benchmark reran it using specific enhancements for that particular test, making them get winning numbers. Then the opponent vendor again reran the test using better enhancements made by a "one-star" guru. And so on to "five-star" gurus.

		\subsubsection{Benchmarketing}
Benchmarketing is a variation of benchmark wars. Due to domain-specific benchmarks there was always a benchmark which rated a particular system the best. Therefore the vendors promoted only benchmarks which highlighted the strengths of their product, trying to make them as a standard. This leaded to a ploriferation of confuse benchmarks.

		\vspace{0.5cm}
Although these phenomenons were drastically reduced by the foundation of the Transaction Processing Performance Council (TPC) in 1988, they are still alive.

		\subsection{Benchmark Differences} \label{categories}
It is now evident how hard is to understand which database is the fastest. Benchmarks cannot be properly used to analyze the performance of several databases and choose simply the best. Every benchmark has a bias, testing some particular aspect of database systems, such as writes, reads, transactions and so on. Beyond, every benchmark operation can be implemented in different ways by the same database: creating a connection for every operation or using a connection pool; use different transaction isolation level; load the whole database in RAM or split it in different hard disk partition; etc. Furthermore the same benchmark, with the same implemention for every database, can show dissimilar results when it runs on different platforms (hardware and software).

All these differences are grouped by three categories:
\begin{enumerate}
	\item Test scenario: reads, writes, etc.
	\item Test implementation: there are different way to implement the same transaction.
	\item Execution platform.
\end{enumerate}
	
		\subsection{The Axiom}
By this reasoning comes out the axiom which will guide the following chapters, and the work behind this thesis:

\label{axiom} \emph{"There is not a slower or a faster database: databases are only slower or faster given a specific set of criteria in a given benchmark"}.

This sentence comes from an article of Bernt Johnsen \cite{Bernt}, who, in response to a benchmark comparing HSQL and Derby, stated that it's easy to make a benchmark, but it is always hard to communicate the meaning of the results. The interpretation depends on too many criteria, so that it's not easy to say which database is the fastest, unless specifying the set of criteria used in the benchmark.
	
		\section{Measures}\label{measures}
While analyzing databases performance it's not possible to measure which database is the best, but we can measure many other parameters, which then can be interpreted in order to choose the database that fit our needs.

When choosing a database, the most important features to evaluate and compare are:

\begin{description}

	\item[Throughput] is the number of transactions per second a database can sustain. This is the most important feature to consider when evaluating a database system. The most representative application scenario to understand the meaning of the throughput is an on-line transaction processing system. This kind of application requires the database to sustain a certain number of transactions per second, based on the number of users, and not every database can suite the needs of the application itself. Another example is real time applications: they require even higher throughput. Therefore it is crucial to understand if a certain database is suitable to an application in terms of transactions per second.
	
	\item[Latency/responsiveness] is the time the database takes to execute a transaction. It can be measured as the inverse of throughput.
	
	\item[File size] of the database image. While traditional DBMS' store objects (tables), indexes and a transaction log file on the file system, in-memory databases, achieving durability, usually store only a journal file containing all the transaction executed on the database. Altought this may seem a small file, it can become very huge, even more than the database image. This measure is interesting whereas each database use different data structures.
	
	\item[RAM usage] is the quantity of RAM a process uses while running. Talking about in-memory database, this can be another way to measure the size of the database image. In addition, this is a critical value to take in mind: IMDBs only works correctly and efficiently under the hypothesis that the RAM is enough to contain the whole database.
	
	\item[CPU load] becomes more important with the increasing number of services that live togheter in the same server. CPU is a precious resource shared by all the processes in a particular computer. While database systems can be usually deployed on a dedicated server, embedded databases live togheter at least with the application which use the database itself. And often in-memory databases are executed in embedded mode.
	
	\item[Disk I/O] shows the usage of the hard disk, the bottleneck for every traditional databases. Altought pure in-memory databases never access to the disk, when adding durability through a transaction log file, disk I/O is a bottleneck for IMDBs too. Therefore this measure is less important than the others because it is possible to take the hypothesis that every databases have the same disk I/O.
	
	\item[Startup time] is the time the database needs to become operational. 
	
	\item[Shutdown time] is the time the database takes to shut down and kill the process.
	
\end{description}
	
	\section{Choosing a Database}
From the previous sections, it's clear how it is difficult to use benchmarks to prove which database is the fastest. Even whit a fair benchmark, which can be very useful to understand the performance of databases, it is still difficult to choose a database: performance is only one factor to consider when evaluating a database \cite{burleson}. 

Other factors to consider are:
\begin{itemize}
	\item The availability of trained DBAs.
	\item The vendor's technical support.
	\item The cost of ownership.
	\item The hardware on which the database will be deployed.
	\item The operating system which will support the database.
\end{itemize}

In other words, it is very difficult to choose the right database for our needs, and, of course, while evaluating databases and benchmarks there is absolutely \emph{no winner}.


\chapter{Test Suite}	
In this chapter we will define the tests used to run a benchmark and to analyze performance. The tests are divided into three categories: base test case, load test case and acid test case.


	\section{Why Define Test Scenarios}
The axiom, previously described in paragraph \ref{axiom} at page \pageref{axiom}, expresses clearly the difficulty to analyze databases' performance and how every result obtained in a given benchmark depends on the specific set of criteria used in the benchmark itself. 

In paragraph \ref{categories} there is also a description of the three major categories in which the criteria fall. The first of these categories is test scenario: different test scenarios may show completely different performance results. It's not possible to avoid this behavior, but we can define clearly every tests so that we will be aware of the differences between them.

	\section{Base Test Case}
Base test case is a collection of very simple tests, which are configured mostly as a race. This is exactly what the major part of benchmarks does, particularly Poleposition \cite{poleposition}.

Every test can execute different read/write operations on the database and all these operations are inside a loop. The word \emph{transaction}, used extensively in the following paragraphs, refers to an execution of the loop, and therefore all the operations inside it.

The key features of these kind of test are:
\begin{itemize}
	\item A fixed number of transaction before the test stop: so tests are configured as a race where every database must execute a certain number of transaction.
	\item A fixed amount of time before the test stop: as an alternative to a fixed number of transaction, every test run for a specific amount of time, executing the maximum number of transactions per second. 
	\item Different kind of object: tests must be able to create/retrieve/update/delete simple flat objects with few fields, or complex flat objects with many fields, or still hierarchical objects, and so on. This feature let us test effectively the performance on the objects used in the domain of our interest.
	\item Single task: to keep these test simple it's better avoid concurrent test, but it is still possible to implement concurrently the different operation executed on the database.
\end{itemize}
	
		\subsection{Race Test: Timed}
We already said base tests are configured inherently as a race. These tests can be used to show the maximum throughput the database can reach doing a particular operation on a specific object. Metaphorically it's like a rocket car in the desert trying to reach its speed limit. In a real application this is rarely useful, but can give us an idea of database's limits. 

In order to create a test scenario we have to define the object/s involved by the test, the operations on which the test loop and when the test should stop. For example:
\begin{itemize}
	\item The object represent a person, with only two fields: an id number and a name. This is a simple flat object. 
	\item The only operation executed by the test is a write operation: every time an object will be added to the database.
	\item 60 seconds is the stop condition of the test: after 60 seconds of execution of writes (objects person inserted in the database) the test stops.
\end{itemize}
Clearly the time is not a significant measure, every test's execution run for the same amount of time: 60 seconds. Instead of the time, a more interesting measure to take is the throughput. This test shows the maximum theoretical value for the throughput, which in a real usage scenario will never be outperformed. 

This test, and its results, are not useful to understand the real performance and potentiality of the database and so to choose the database for our needs, but it can be used in an early stage to reduce the databases' set which will be analyzed extensively with further tests. In other words we can throw away every databases whose maximum throughput is not enough to our needs.

		\subsection{Race Test: Transactions}
This test is really similar to the previous test. The only difference is in the stop condition:
\begin{itemize}
	\item The object represent a person, with only two fields: an id number and a name. This is a simple flat object (same as before). 
	\item The only operation executed by the test is a write operation: every time an object will be added to the database (same as before).
	\item 1.000.000 of transactions is the stop condition of the test: when 1.000.000 of writes are executed (objects person inserted in the database) the test stops.
\end{itemize}
While in the previous test the duration was meaningless, this time, like in a race, it shows which database is the fastest (the winner of the race). Despite this, the duration is still of little use. Also for the throughput the same considerations made before are still valid.

But this new test is useful also to take other interesting measures, such as the file size, whose meaning has already been exaplined in paragraph \ref{measures}. We can evaluate how the file size increase with the number of objects (person) inserted in the database. This is because, differently to the previous one, every test's execution perform a fixed amount of transactions. 

	\section{Load Test Case}
Restrictions introduced by base tests are very evident although they can be usefull in certain situation and for simple and fast approximate results. These restrictions are due to the impossibility to simulate real complex application scenario. Substantially the main restrictions are two:
\begin{enumerate}
	\item Real application scenario are rarely single thread/task: to stick to the axiom at paragraph \ref{axiom}, the best way to understand which database fits our needs is to make test scnerios most realistic as possible.
	\item The second restriction result from the first:trying to make test scenarios more realistic, it is necessary to introduce some sort of control for the throughput. When a test stress the database engine to its maximum level, some other mechanisms may not work properly, such as the garbage collector, caching, indexing, etc. In addition, when a test concurrently accesses the database, this restriction is even more evident: if a thread is not limited in its transactions per second, it may lower the performance of other threads.
\end{enumerate}

Base test case offer very important and useful results, but when the need to test the average load of a real application, of our interest, they are not enough. These restrictions are solved by load test case, which allow a better simulation of the application scenario. The key features of these tests, differently by base tests, are:
\begin{itemize}
	\item Multi task: there will be not only a combination of read/write/update/delete in one transaction executed synchronizedly, but in more transactions running concurrently against the database.
	\item A bond on the throughput: for every transaction it is possible to specify the maximum throughput, if the database can reach it, in order to simulate real application usage.
\end{itemize}

From these features comes the name "load test case", which means the simulation of an average, or specific, load on a certain database. This idea, and the need of this kind of tests,  can simply be exaplined by a Bernt's metaphor, who has been already cited for the axiom at paragraph \ref{axiom}. The metaphor is:

\emph{"I don't buy the fastest car in the market. I don't even buy the fastest car I can afford. I buy a car I can afford that fits my needs... e.g. drive to the northern Norway with 2 grown-ups, 3 kids, a kayak, sleeping bags, a large tent, glacier climbing equipment, food, clothes and so on. Can't do that with a Lamborghini"}\cite{Bernt}.

This metaphor explain exactly the need not for the fastest database, but for a database which can sustain the load of the application without any complexity during its normal functioning, such as a database snapshot freezing all writes operation, or a bug/memory leak making the database crash.

		\subsection{Real Time Prepaid System}
Keeping these features in mind, we want to create a load test case to analyze exactly which performance  a database system  offers for a practical use case: a real time prepaid system, such as a telephone company. Basically the test is the concurrent execution of 3 different task, each involving complex transactions: check balance, accounts handling, and manage calls and services. 

We start describing the domain objects used by this test, and the we pass to analyze the three different task involved.
		
			\subsubsection{Domain Objects}
Domain objects involved by this test are typical for a telephone company who handle telephone calls for every person, which is rappresented by an account, and which can access to many service through a web application, offered by the company for its customers. There are four domain objects used byt this test:
\begin{description}
	\item[Account]: this object represent a customer in the real time prepaid system. It contains all the typical information needed by a telephone company, such as the balance, the type of subcription, etc. In other words, this is a complex flat object, that is having many private field but no hierarchies. In a real application there are milions of accounts instantiated in the database, one for every customer. This dimesion is also very important to make the simulation as real as possible, in order to get realistic results.
	\item[Msisdn]: it is the unique number which identify a subscription is a GSM or UMTS mobile network, it is the telephone number. Each msisdn object is linked to an account. Therefore there are also milions of msisdn objects in the database, referring to a real application.
	\item[Webuser]: this represents the customer logged in the company web site. It contains the username and the password to access to web services. In common with msisdn, it is linked to an account too, and both are simple flat objects: a very simple object with few fields.
	\item[Session]: when a customer start a call, an object session is created, and it keep all the information about the call which is going on, until the call ends. After the call this object doesn't is deleted. A session contain the start time of the call represented, and the time of the last unit. Each session references to an account, which started the call. But there are not as many sessions as accounts, not everyone will start a call in the same time, except New Year's Day!
\end{description}

			\subsubsection{Check Balance Task}
This task simulate a customer checking his residual balance for the prepaid card, a SIM in the case of the telephone company. First, the customer logins in the website or calls the dedicated number, and then read/listen his balance.

Each task, as already explained, corresponds to a transaction composed by different operations. This leads to illustrate how the task work in terms of operations executed:
\begin{description}
	\item[Read] randomly the msisdn if the customer makes a call or the webuser if he access to the website.
	\item[Read] the account referenced by the msisdn or webuser, and then check the residual balance, a simple account's field.
\end{description}
So this task executes a total of two read for every transaction.

Another important parameter to make this task a parte of a load test is the amount of transactions per second this task should sustain. The average throughput for check balance task, considering there are milions of accounts in the database, is about ten transactions per second.

			\subsubsection{Accounts Handling Task}
The account handling task simulates the creation of a new subscription by a customer, and so the creation of the account object, and after the relative msisdn and webuser. 

The operations involved by this task are:
\begin{description}
	\item[Write] of a new account: all informations of a customer are inserted in the system by creating a new account object.
	\item[Write] of a new msisdn, cointaining the telephone number of the new subscription.
	\item[Write] of a new webuser with username and password for the customer.
\end{description}
To sum up, this task executes three write on the database for every transaction.

To simulate a real scenario, the average throughput is about ten transactions per second.

			\subsubsection{Manage Calls and Services Task}
This task simulates a call started by a customer. After checking the balance, and therefore if the account is allowed to start a call or use a service, a new session is created and updated every 30 seconds, the unit time. When the call ends, about two minutes, the session is deleted.

In terms of operations executed on the database, it can be described as follow:
\begin{description}
	\item[Read] 
	\item[Read] 
	\item[Write] 
	\item[Update] 
	\item[Delete] 
\end{description}

   1. retrieve 1 msisdn or 1 webuser
   2. navigate to the acocunt object
   3. check if the user has the permission for the operation requested
   4. open a call session, updating the acocunt
   5. refresh the call session every 30 seconds, updating the account
   6. end the call session, deleting it 



			\subsubsection{Considerazioni finali e riepilogo}
			
	\section{Acid Test Case}
Citazione tpc pdf pagina 46 \cite{TPC-C}.

\chapter{Database Benchmark Softwares Overview}
	\section{Benchmark Requirements}
Desirable attributes ... list of attributes \cite{tpc/sigmoid}.
	
	\section{The Open Source Database Benchmark}
	
	\section{Transaction Processing Performance Council}
\emph{Benchmark results are highly dependent upon workload, specific application requirements, and systems design and implementation. Relative system performance will vary as a result of these and other factors. Therefore, TPC-C should not be used as a substitute for a \bfseries{specific customer application} benchmarking when critical capacity planning and/or product evaluation decisions are contemplated}\cite{TPC-C}.
	
	\section{Apache JMeter}
	\section{Poleposition}
	Poleposition fa schifo! \cite{poleposition}!!!
	
	
\chapter{The New In-Memory Database Benchmark Application}
	\section{Functional View}
	\section{Development View}
	\section{Plug-In Architecture}

\chapter{Results' Analysis}
